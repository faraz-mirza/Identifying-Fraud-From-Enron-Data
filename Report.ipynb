{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Identifying Fraud From Enron Data </h1>  \n",
    "\n",
    "<h3>By Faraz Mirza</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Goals</h2>\n",
    "<li>Understand the dataset and problem associated with it.</li>\n",
    "<li>examine the tools which help us describe and visualize the data.</li>\n",
    "<li>perform data wrangling e.g. outlier cleanup.</li>\n",
    "<li>feature selection and scaling.</li>\n",
    "<li>algorithm selection and tuning.</li>\n",
    "<li>validation and classic mistakes.</li>\n",
    "<li>evaluation metrics and interpretation of algorithm's performance.</li>\n",
    "<li>create useful artifacts - report of our findings, plots and code.</li>\n",
    "\n",
    "I am going to use Machile Learning classification techniques and models that I learned during the course to determine if someone is person of interest. I will divide my data into training and test subsets, using training data to train my model and then using test subset to predict POI and use evalualtion metrics to determine the algorithm's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background information\n",
    "\n",
    "In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, there was a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for to executives.\n",
    "\n",
    "In this project, I will be utilizing scikit-learn and machine learning methodologies that I study with Professor Sabastian and Katie. Using features from financial data, email data, and labeled data, I will try to setup identifier a \"person of interest\" (POI) to detect and predict reprehensible persons. Katie defined POIs as peopelse who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.\n",
    "\n",
    "The dataset contained 146 records with 14 financial features, 6 email features, and 1 labeled feature (POI). Of the 146 records, 18 were labeled as persons of interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "I used matplotlib to plot a scatter graph between Salary and Bonus features to look for outliers. The First Outlier I detected was \"TOTAL\". This was an extreme outlier for most numerical features, as it was likely a spreadsheet artifact.  \n",
    "\n",
    "<img src=\"outlier_salary_bonus.png\" >\n",
    "\n",
    "After I removed the \"TOTAL\" record. My scatter plot looked like this:\n",
    "\n",
    "<img src=\"Figure_1.png\" >\n",
    "\n",
    "Many other outliers showed up as the range of the graph got closer. I removed the top 5% values based on salary and bonuses but many of those people were marked POI. Hence, I did not remove these outliers in my final features list because they would negatively affect the results of my algorithm.\n",
    "\n",
    "I used a different approach then to clean the data. I made a list that keeps a count of number of features with missing values of each person. There were a lot of people with missing features. Since I had a limited data already, I couldn't afford to discard them all. I set the threshold on 17 or above missing features and therefore, was able to remove these following people:\n",
    "\n",
    "['WODRASKA JOHN', 'WHALEY DAVID A', 'WROBEL BRUCE', 'SCRIMSHAW MATTHEW', 'LOCKHART EUGENE E', 'THE TRAVEL AGENCY IN THE PARK', 'GRAMM WENDY L']\n",
    "\n",
    "As their NaN values were replaced with zero during the Feature Formatting process, these data points were ruining the results rather than contributing to the training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Engineered\n",
    "\n",
    "I came up with 3 new Features.\n",
    "\n",
    "1. poi_ratio: Ratio between messages connected with poi and all messages.\n",
    "2. poi_to_ratio: The fraction of all emails that a person sent that were addressed to persons of interest.\n",
    "3. poi_from_ratio: The fraction of all emails to a person that were sent from a person of interest.\n",
    "\n",
    "My idea behind using these features were to investigate the strength of communication link between poi and compare it between poi and other people to determine whether they can be considered as poi as well. It was found certainly there is strong communcation link between the pois then between poi and non-pois.\n",
    "\n",
    "Out of these three, KBest choose poi_from_ratio in top 10 features. I ran the algorithm with as well as without this created feature. Following are the scores I got:\n",
    "\n",
    "<b>Scores with Original Features:</b>  \n",
    "Accuracy: 0.81486\tPrecision: 0.22794\tRecall: 0.12400\tF1: 0.16062\tF2: 0.13644\n",
    "\n",
    "<b>Scores with New Feature(s) Included:</b>  \n",
    "Accuracy: 0.84836\tPrecision: 0.45720\tRecall: 0.32850\tF1: 0.38231\tF2: 0.34810\n",
    "\n",
    "So as you can clearly see, the new feature(s) have a positive impact on the final Algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "To help me select the best features to train my identidier, I leveraged the use of the scikit-learn's SelectKBest module to select the 10 most influential features. Their associated scores are listed in the table below:\n",
    "\n",
    "\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-yw4l\">Feature</th>\n",
    "    <th class=\"tg-yw4l\">Score</th>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">exercised_stock_options</td>\n",
    "    <td class=\"tg-yw4l\">23.45</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">total_stock_value</td>\n",
    "    <td class=\"tg-yw4l\">22.79</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">bonus</td>\n",
    "    <td class=\"tg-yw4l\">19.46</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">salary</td>\n",
    "    <td class=\"tg-yw4l\">16.86</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">poi_from_ratio</td>\n",
    "    <td class=\"tg-yw4l\">15.25</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">deferred_income</td>\n",
    "    <td class=\"tg-yw4l\">10.77</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">long_term_incentive</td>\n",
    "    <td class=\"tg-yw4l\">9.17</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">restricted_stock</td>\n",
    "    <td class=\"tg-yw4l\">8.55</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">total_payments</td>\n",
    "    <td class=\"tg-yw4l\">8.27</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">shared_receipt_with_poi</td>\n",
    "    <td class=\"tg-yw4l\">7.80</td>\n",
    "    \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason I chose 10 features is due to the limited size of enron dataset to train a classifier. Including higher number of features will only increase the chances of overfitting rather than improving results. To support my statement, I ran the tester.py on 10-best features and 11-best features. Here are the results:\n",
    "\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-yw4l\">Decision Tree Classifier</th>\n",
    "    <th class=\"tg-yw4l\">Presicion</th>\n",
    "    <th class=\"tg-yw4l\">Recall</th>\n",
    "    <th class=\"tg-yw4l\">Accuracy</th>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">10 Best Features</td>\n",
    "    <td class=\"tg-yw4l\">0.3687</td>\n",
    "    <td class=\"tg-yw4l\">0.299</td>\n",
    "    <td class=\"tg-yw4l\">0.827</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">11 Best Features</td>\n",
    "    <td class=\"tg-yw4l\">0.3586</td>\n",
    "    <td class=\"tg-yw4l\">0.288</td>\n",
    "    <td class=\"tg-yw4l\">0.825</td>\n",
    "\n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Feature Scaling is one important tool in Machine Learning. It's about how we normalize the range of each of our feature so that it can't dominate from one to another. There are 2 types of features in Enron Dataset, i.e Financial Features and Email Features. While financial features are all in dollars and big values, Number of emails are mostly in hundreds or low thousands. I used Scikit-learn's MinMaxScaler() function to normalize all features so that features like Salary or Bonus will not dmoniate the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Selection\n",
    "\n",
    "I ran tests on the following algorithms to check their precision, recall and accuracy in order to pick algorithm to work with. All algorithms were run on their default parameters.\n",
    "\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-yw4l\">Classifier</th>\n",
    "    <th class=\"tg-yw4l\">Presicion</th>\n",
    "    <th class=\"tg-yw4l\">Recall</th>\n",
    "    <th class=\"tg-yw4l\">Accuracy</th>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">GaussianNB</td>\n",
    "    <td class=\"tg-yw4l\">0.37405</td>\n",
    "    <td class=\"tg-yw4l\">0.29550</td>\n",
    "    <td class=\"tg-yw4l\">0.82871</td>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">SVM</td>\n",
    "    <td class=\"tg-yw4l\">0.0</td>\n",
    "    <td class=\"tg-yw4l\">0.0</td>\n",
    "    <td class=\"tg-yw4l\">-</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">Decision Tree</td>\n",
    "    <td class=\"tg-yw4l\">0.32071</td>\n",
    "    <td class=\"tg-yw4l\">0.29650</td>\n",
    "    <td class=\"tg-yw4l\">0.80979</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">Ada Boost</td>\n",
    "    <td class=\"tg-yw4l\">0.32342</td>\n",
    "    <td class=\"tg-yw4l\">0.30450</td>\n",
    "    <td class=\"tg-yw4l\">0.80964</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">Random Forest</td>\n",
    "    <td class=\"tg-yw4l\">0.42837</td>\n",
    "    <td class=\"tg-yw4l\">0.15550</td>\n",
    "    <td class=\"tg-yw4l\">0.84971</td>\n",
    "\n",
    "  </tr>\n",
    "\n",
    "\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above result, I picked up DecisionTreeClassifier to work with. SVM model failed on this data. Naive Bayes got a good precision but recall does not meet the requirement and there's no room for tuning as well. RandomForestClassifier got the best Precision but very low recall to be consired. Ada Boost Classifier got slighly better scores than Decision Tree but it's really slow and would take a lot of time to tune it's parameters therefore I chose Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q4. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the Algorithm\n",
    "\n",
    "Machine learning algorithms are parameterized just like functions and we can tweek these parameters to influence the outcome of the learning process. These parameters start off set to their default values but once we start modifying these parameters around is known as ‘tuning the algorithm’.\n",
    "\n",
    "The objective of algorithm tuning is to find the best point or points in the problem where performance can be optimum. The more tuned the parameters of an algorithm, the more biased the algorithm will be to the training data and test harness. This strategy can be effective, but it can also lead to more fragile models that overfit the test harness and don’t perform as well in practice.\n",
    "\n",
    "I approached the problem by using automated methods that impose a grid using scikit-learn's grid_search.GridSearchCV.\n",
    "\n",
    "I gave the following range of parameters to Grid search to try all the possible combinations and pick the best one:\n",
    "\n",
    "parameters = {'criterion':('gini', 'entropy'), 'max_depth':[1,2,3,4,5], 'min_samples_split': [2,5,6,7,8,9,10,15,20,30,50], 'min_samples_leaf':[1,2,3,4,5], 'random_state':[1,2,3,4,5] }\n",
    "\n",
    "Grid search provided the following parameters:\n",
    "\n",
    "{'min_samples_split': 2, 'random_state': 2, 'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 4}\n",
    "\n",
    "I ran the DecisionTreeClassifier using these parameters and I got the following scores:\n",
    "\n",
    "Accuracy: 0.84836\tPrecision: 0.45720\tRecall: 0.32850\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q5. What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Validation comprises set of techniques to make sure our model generalizes with the remaining part of the dataset. A classic mistake, which was briefly mistaken by me, is over-fitting where the model performed well on training set but have substantial lower result on test set. In order to overcome such classic mistake, we can conduct cross-validation. I used StratifiedSuffleSplit which is in file tester.py. Main reason why I used this rather than other splitting techniques available is due to the nature of our dataset, which is extremely small with only 18 Persons of Interest. A single split into a training and test set would not give a better estimate of error accuracy. Therefore, we need to randomly split the data into multiple trials while keeping the fraction of POIs in each trials relatively constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Q6. Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics  \n",
    "\n",
    "\n",
    "The main evaluation metrics utilized were precision and recall. Their following formulas are:\n",
    "\n",
    "Precision: TP/(TP+FP)  \n",
    "Recall: TP/(TP+FN)\n",
    "\n",
    "Precision captures the ratio of true positives to the records that are actually POIs. My algorithm's Precision came out to be 0.45720, which means from the people classified as POIs by the model, 45.72% of them were actual POIs. \n",
    "\n",
    "Recall captures the ratio of true positives to the records flagged as POIs, which describes sensitivity. My algorithm's Recall came out to be 0.32850, which means if there were 18 POI's in the dataset, it was able to correctly identify 6(32.85%) of them. \n",
    "\n",
    "Due to the unbalanced nature of the dataset (few POIs), accuracy is certainly not a good metric, i.e. if 'non-POI' had been predicted for all records, an accuracy of 87.4% would have been achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References  \n",
    "<br>\n",
    "\n",
    "<li>Introduction to Machine Learning (Udacity)</li>\n",
    "<li>scikit-learn Documentation </li>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
